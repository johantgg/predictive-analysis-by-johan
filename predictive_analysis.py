# -*- coding: utf-8 -*-
"""Predictive_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18h0HKY9ZwJf3-FbpSXmiREVZ-YZw5HFL

# Credit Card Prediction Analysis

# Konteks

Kartu skor kredit adalah metode pengendalian risiko yang umum di industri keuangan. Ini menggunakan informasi pribadi dan data yang dikirimkan oleh pemohon kartu kredit untuk memprediksi kemungkinan gagal bayar di masa depan dan pinjaman kartu kredit. Bank dapat memutuskan apakah akan menerbitkan kartu kredit kepada pemohon. Nilai kredit dapat mengukur besarnya risiko secara objektif.

Secara umum, kartu skor kredit didasarkan pada data historis. Setelah menghadapi fluktuasi ekonomi yang besar. Model masa lalu mungkin kehilangan kekuatan prediksi aslinya. Model logistik adalah metode umum untuk penilaian kredit. Karena Logistic cocok untuk tugas klasifikasi biner dan dapat menghitung koefisien setiap fitur. Untuk memudahkan pemahaman dan pengoperasian, kartu skor akan mengalikan koefisien regresi logistik dengan nilai tertentu (misalnya 100) dan membulatkannya.

Saat ini, dengan berkembangnya algoritma pembelajaran mesin. Metode yang lebih prediktif seperti Boosting, Random Forest, dan Support Vector Machines telah diperkenalkan ke dalam penilaian kartu kredit. Namun, metode-metode tersebut sering kali tidak memiliki transparansi yang baik. Mungkin sulit untuk memberikan alasan penolakan atau penerimaan kepada pelanggan dan regulator.



# Tugas
Buat model pembelajaran mesin untuk memprediksi apakah pelamar adalah klien 'baik' atau 'buruk', berbeda dari tugas lainnya, definisi 'baik' atau 'buruk' tidak diberikan. Anda harus menggunakan beberapa teknik, seperti analisis vintage untuk membuat label Anda. Selain itu, masalah ketidakseimbangan data merupakan masalah besar dalam tugas ini.

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Extracting data using two data sources

### Membaca Dataset
Membaca data `application_record.csv` dan `credit_record.csv` untuk eksplorasi awal.
"""

app = pd.read_csv("application_record.csv")
crecord = pd.read_csv("credit_record.csv")

"""* Using different methods to understand data
* data is complex and both dataset need some kind of transformation before analysis
* datasets are indivudally dealt with and then eventually compiled using joins
"""

app.info()

crecord.info()

app['ID'].nunique() # the total rows are 438,557. This means it has duplicate

crecord['ID'].nunique()

len(set(crecord['ID']).intersection(set(app['ID']))) # checking to see how many records match in two datasets

"""### Visualisasi Missing Value
Plot heatmap untuk melihat distribusi nilai kosong dalam dataset.

![missing-value-heatmap](missing_heatmap.png)
"""

sns.heatmap(app.isnull()) # checking for null values. Seems like occupation_type has many
plt.savefig('missing_heatmap1.png')

"""### Visualisasi Missing Value
Plot heatmap untuk melihat distribusi nilai kosong dalam dataset.

![missing-value-heatmap](missing_heatmap.png)
"""

sns.heatmap(crecord.isnull()) # checking for null values. All good here!
plt.savefig('missing_heatmap2.png')

"""### Menghapus Duplikat
Menghapus data duplikat berdasarkan kolom `ID`.
"""

app = app.drop_duplicates('ID', keep='last')
# we identified that there are some duplicates in this dataset
# we will be deleting those duplicates and will keep the last entry of the ID if its repeated.

app.drop('OCCUPATION_TYPE', axis=1, inplace=True)
#we identified earlier that occupation_type has many missing values
# we will drop this column

ot = pd.DataFrame(app.dtypes =='object').reset_index()
object_type = ot[ot[0] == True]['index']
object_type
#we are filtering the columns that have non numeric values to see if they are useful

num_type = pd.DataFrame(app.dtypes != 'object').reset_index().rename(columns =  {0:'yes/no'})
num_type = num_type[num_type['yes/no'] ==True]['index']
#HAVE CREATED SEPARATE LIST FOR NUMERIC TYPE INCASE IT WILL BE NEEDED IN FURTHER ANALYSIS
# IT IS NEEDED IN FURTHER ANALYSIS

a = app[object_type]['CODE_GENDER'].value_counts()
b = app[object_type]['FLAG_OWN_CAR'].value_counts()
c = app[object_type]['FLAG_OWN_REALTY'].value_counts()
d = app[object_type]['NAME_INCOME_TYPE'].value_counts()
e = app[object_type]['NAME_EDUCATION_TYPE'].value_counts()
f = app[object_type]['NAME_FAMILY_STATUS'].value_counts()
g = app[object_type]['NAME_HOUSING_TYPE'].value_counts()

print( a,"\n",b,'\n', c, '\n', d, '\n', e, '\n', f, '\n', g)

#this is just to see what each column is.
#It seems that all of them are important since there is very fine classifcation in each column.
# their effectiveness cannot be judged at this moment so we convert all of them to numeric values.

"""### Encoding Variabel Kategorikal
Variabel kategori diubah ke bentuk numerik menggunakan `LabelEncoder`.
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for x in app:
    if app[x].dtypes=='object':
        app[x] = le.fit_transform(app[x])
# we have transformed all the non numeric data columns into data columns
# this method applies 0,1.. classification to different value types.

app.head(10)

app[num_type].head()
# We will look at numeric columns and see if there is anything that needs to be changed.

# prompt: coba buat codingan visualisasi dari data preparation nya

# Visualisasi Data
# Histogram untuk kolom numerik
app[num_type].hist(figsize=(10, 10))
plt.tight_layout()
plt.show()

# Countplot untuk kolom kategorikal yang sudah di-encode
for col in object_type:
  plt.figure(figsize=(6, 4))
  sns.countplot(x=col, data=app)
  plt.title(f'Distribution of {col}')
  plt.show()

fig, ax= plt.subplots(nrows= 3, ncols = 3, figsize= (14,6))

sns.scatterplot(x='ID', y='CNT_CHILDREN', data=app, ax=ax[0][0], color= 'orange')
sns.scatterplot(x='ID', y='AMT_INCOME_TOTAL', data=app, ax=ax[0][1], color='orange')
sns.scatterplot(x='ID', y='DAYS_BIRTH', data=app, ax=ax[0][2])
sns.scatterplot(x='ID', y='DAYS_EMPLOYED', data=app, ax=ax[1][0])
sns.scatterplot(x='ID', y='FLAG_MOBIL', data=app, ax=ax[1][1])
sns.scatterplot(x='ID', y='FLAG_WORK_PHONE', data=app, ax=ax[1][2])
sns.scatterplot(x='ID', y='FLAG_PHONE', data=app, ax=ax[2][0])
sns.scatterplot(x='ID', y='FLAG_EMAIL', data=app, ax=ax[2][1])
sns.scatterplot(x='ID', y='CNT_FAM_MEMBERS', data=app, ax=ax[2][2], color= 'orange')

"""There are outliers in 3 columns.
1. CNT_CHILDREN
2. AMT_INCOME_TOTAL
3. CNT_FAM_MEMBERS

* We need to remove these outliers to make sure they do not affect our model results.
* We will now remove these outliers.

### Menghapus Outlier
Outlier dihapus menggunakan metode IQR pada kolom `CNT_CHILDREN`, `AMT_INCOME_TOTAL`, dan `CNT_FAM_MEMBERS`.
"""

# FOR CNT_CHILDREN COLUMN
q_hi = app['CNT_CHILDREN'].quantile(0.999)
q_low = app['CNT_CHILDREN'].quantile(0.001)
app = app[(app['CNT_CHILDREN']>q_low) & (app['CNT_CHILDREN']<q_hi)]

# FOR AMT_INCOME_TOTAL COLUMN
q_hi = app['AMT_INCOME_TOTAL'].quantile(0.999)
q_low = app['AMT_INCOME_TOTAL'].quantile(0.001)
app= app[(app['AMT_INCOME_TOTAL']>q_low) & (app['AMT_INCOME_TOTAL']<q_hi)]

#FOR CNT_FAM_MEMBERS COLUMN
q_hi = app['CNT_FAM_MEMBERS'].quantile(0.999)
q_low = app['CNT_FAM_MEMBERS'].quantile(0.001)
app= app[(app['CNT_FAM_MEMBERS']>q_low) & (app['CNT_FAM_MEMBERS']<q_hi)]

fig, ax= plt.subplots(nrows= 3, ncols = 3, figsize= (14,6))

sns.scatterplot(x='ID', y='CNT_CHILDREN', data=app, ax=ax[0][0], color= 'orange')
sns.scatterplot(x='ID', y='AMT_INCOME_TOTAL', data=app, ax=ax[0][1], color='orange')
sns.scatterplot(x='ID', y='DAYS_BIRTH', data=app, ax=ax[0][2])
sns.scatterplot(x='ID', y='DAYS_EMPLOYED', data=app, ax=ax[1][0])
sns.scatterplot(x='ID', y='FLAG_MOBIL', data=app, ax=ax[1][1])
sns.scatterplot(x='ID', y='FLAG_WORK_PHONE', data=app, ax=ax[1][2])
sns.scatterplot(x='ID', y='FLAG_PHONE', data=app, ax=ax[2][0])
sns.scatterplot(x='ID', y='FLAG_EMAIL', data=app, ax=ax[2][1])
sns.scatterplot(x='ID', y='CNT_FAM_MEMBERS', data=app, ax=ax[2][2], color= 'orange')

crecord['Months from today'] = crecord['MONTHS_BALANCE']*-1
crecord = crecord.sort_values(['ID','Months from today'], ascending=True)
crecord.head(10)
# we calculated months from today column to see how much old is the month
# we also sort the data according to ID and Months from today columns.

"""### Transformasi Nilai STATUS
Nilai `STATUS` diubah ke format biner agar sesuai dengan tugas klasifikasi.
"""

crecord['STATUS'].value_counts()
# performed a value count on status to see how many values exist of each type

"""### Transformasi Nilai STATUS
Nilai `STATUS` diubah ke format biner agar sesuai dengan tugas klasifikasi.
"""

crecord['STATUS'].replace({'C': 0, 'X' : 0}, inplace=True)
# Fill any remaining NaN values with 0 before converting to integer
crecord['STATUS'].fillna(0, inplace=True)
crecord['STATUS'] = crecord['STATUS'].astype('int')
crecord['STATUS'] = crecord['STATUS'].apply(lambda x:1 if x >= 2 else 0)

"""### Transformasi Nilai STATUS
Nilai `STATUS` diubah ke format biner agar sesuai dengan tugas klasifikasi.
"""

crecord['STATUS'].value_counts(normalize=True)

crecordgb = crecord.groupby('ID').agg(max).reset_index()
crecordgb.head()
#we are grouping the data in crecord by ID so that we can join it with app

df = app.join(crecordgb.set_index('ID'), on='ID', how='inner')
df.drop(['Months from today', 'MONTHS_BALANCE'], axis=1, inplace=True)
df.head()
# no that this is joined, we will solve over sampling issue

"""df.info() # checking for number of rows.
# there are 9516 rows.
"""

df.info() # checking for number of rows.
# there are 9516 rows.

X = df.iloc[:,1:-1] # X value contains all the variables except labels
y = df.iloc[:,-1] # these are the labels

"""### Split Data
Membagi data ke dalam set latih dan uji dengan rasio 70:30.
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)
# we create the test train split first

"""### Scaling
Melakukan normalisasi fitur numerik menggunakan MinMaxScaler.
"""

from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_scaled = pd.DataFrame(mms.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(mms.transform(X_test), columns=X_test.columns)
# we have now fit and transform the data into a scaler for accurate reading and results.

"""### Menangani Ketidakseimbangan Kelas
Menggunakan SMOTE untuk oversampling data minoritas.
"""

from imblearn.over_sampling import SMOTE
oversample = SMOTE()
X_balanced, y_balanced = oversample.fit_resample(X_scaled, y_train)
X_test_balanced, y_test_balanced = oversample.fit_resample(X_test_scaled, y_test)
# we have addressed the issue of oversampling here

y_train.value_counts()

y_balanced.value_counts()

y_test.value_counts()

y_test_balanced.value_counts()

"""* We notice in the value counts above that label types are now balanced
* the problem of oversampling is solved now
* we will now implement different models to see which one performs the best

### Model Development
Melatih beberapa model klasifikasi termasuk Logistic Regression, SVM, Decision Tree, Random Forest, dan XGBoost.

Fungsi evaluate_model()
"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def evaluate_model(name, model, X_train, X_test, y_train, y_test):
    print(f"\n=== {name} ===")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

"""## 🎯 Evaluasi Model: Logistic Regression

"""

from sklearn.linear_model import LogisticRegression

evaluate_model("Logistic Regression", LogisticRegression(),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

"""📌 KNN
## 🎯 Evaluasi Model: K-Nearest Neighbors (KNN)

"""

from sklearn.neighbors import KNeighborsClassifier

evaluate_model("K-Nearest Neighbors", KNeighborsClassifier(),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

"""## 🎯 Evaluasi Model: Support Vector Machine (SVM)

"""

from sklearn.svm import SVC

evaluate_model("SVM", SVC(),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

"""## 🎯 Evaluasi Model: Decision Tree

"""

from sklearn.tree import DecisionTreeClassifier

evaluate_model("Decision Tree", DecisionTreeClassifier(),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

"""## 🎯 Evaluasi Model: Random Forest

"""

from sklearn.ensemble import RandomForestClassifier

evaluate_model("Random Forest", RandomForestClassifier(),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

"""## 🎯 Evaluasi Model: XGBoost

"""

from xgboost import XGBClassifier

evaluate_model("XGBoost",
               XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
               X_balanced, X_test_balanced, y_balanced, y_test_balanced)

from graphviz import Digraph

dot = Digraph(comment='Model Workflow')

# Nodes
dot.node('A', 'Load Data\n(application_record, credit_record)')
dot.node('B', 'Data Cleaning & Preprocessing\n(Handle Duplicates, Missing Values)')
dot.node('C', 'Feature Engineering\n(Months from today)')
dot.node('D', 'Data Transformation\n(Label Encoding, Outlier Removal)')
dot.node('E', 'Merge Data')
dot.node('F', 'Target Variable Creation\n(STATUS Transformation)')
dot.node('G', 'Split Data\n(Train/Test)')
dot.node('H', 'Feature Scaling\n(MinMaxScaler)')
dot.node('I', 'Handle Class Imbalance\n(SMOTE)')
dot.node('J', 'Model Training\n(Logistic Regression, SVM, etc.)')
dot.node('K', 'Model Evaluation\n(Accuracy)')
dot.node('L', 'Best Model Selection\n(XGBoost)')
dot.node('M', 'Prediction')

# Edges
dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH', 'HI', 'IJ', 'JK', 'KL', 'LM'])

# Render the graph
dot.render('model_workflow', view=True, cleanup=True)

"""### Evaluasi Model Terbaik
Menggunakan classification report untuk menilai performa model terbaik (XGBoost).
"""

from sklearn.metrics import classification_report
print(classification_report(y_test_balanced, prediction))

# prompt: buatkan codingan visualisasi evaluation metrics

# Visualisasi Classification Report
report = classification_report(y_test_balanced, prediction, output_dict=True)
df_report = pd.DataFrame(report).transpose()

plt.figure(figsize=(10, 6))
sns.heatmap(df_report.iloc[:-1, :].T, annot=True, cmap="Blues", fmt=".2f")
plt.title('Classification Report Heatmap')
plt.ylabel('Metrics')
plt.xlabel('Classes')
plt.show()

# Optional: Bar plot for precision, recall, f1-score
metrics = ['precision', 'recall', 'f1-score']
classes = ['0', '1']

fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)

for i, metric in enumerate(metrics):
    data = [df_report.loc[classes[0], metric], df_report.loc[classes[1], metric]]
    sns.barplot(x=classes, y=data, ax=axes[i], palette='viridis')
    axes[i].set_title(f'{metric.capitalize()}')
    axes[i].set_xlabel('Class')
    axes[i].set_ylabel(metric.capitalize() if i == 0 else "")
    axes[i].set_ylim(0, 1)

plt.tight_layout()
plt.show()

# prompt: buatkan kodingan hasil evaluasi model tadi dengan bentuk visualisasi

model_names = list(classifiers.keys())

plt.figure(figsize=(10, 6))
plt.bar(model_names, train_scores, label='Training Accuracy', color='skyblue')
plt.bar(model_names, test_scores, label='Testing Accuracy', color='orange')
plt.ylabel('Accuracy')
plt.title('Training and Testing Accuracy of Different Models')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

# For the best model (XGBoost), visualize the classification report
# Extract precision, recall, f1-score for class 0 and 1
report_dict = classification_report(y_test_balanced, prediction, output_dict=True)
df_report = pd.DataFrame(report_dict).transpose()

# Keep only the rows for class 0, class 1, and 'macro avg' or 'weighted avg' if needed
metrics_to_plot = ['0', '1'] # Adjust if you want to include averages

df_plot = df_report.loc[metrics_to_plot, ['precision', 'recall', 'f1-score']]

df_plot.plot(kind='bar', figsize=(10, 6))
plt.title('Classification Report Metrics for XGBoost Model')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.legend(title='Metrics')
plt.tight_layout()
plt.show()